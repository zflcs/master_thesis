\chapter{相关技术研究}

\section{任务模型}


操作系统任务模型的演化历程深刻反映了计算范式的变革与软硬件协同设计的迭代进步，其发展轨迹始终与底层硬件架构革新及并发编程理论突破紧密耦合。

\subsection{批处理作业模型}

在早期的操作系统中，任务以物理作业卡为载体形成离散的批处理单元，其建模遵循冯·诺伊曼提出的串行执行假设 \cite{VonNeumann}，在内存中仅存放一个作业，每个作业作为封闭的地址空间实体独占系统资源，按照顺序自动执行磁带上的作业，虽然减少了人工操作时间，但 CPU 在等待 I/O 操作时仍会闲置，导致整体效率受限。这一问题促使多道程序设计（Multiprogramming）概念的诞生，允许多个程序同时驻留内存并共享 CPU 资源，当某个程序因 I/O 请求暂停时，CPU 可立即切换执行其他程序，显著提升了资源利用率和系统吞吐量，但多个程序之间没有形成保护边界。并且，批处理作业模型也因为缺乏人机交互性，在程序调试和实时响应方面存在局限。
    
批处理作业的核心思想在于将任务或数据集合处理，通过减少任务切换开销和资源闲置时间来优化系统的整体效率。这种思想不仅应用于早期操作系统，也深刻影响了现代计算系统设计，例如在大数据场景中通过批量处理提升吞吐量，甚至在某些场景下通过聚合操作降低宏观层面的延迟。

\subsection{进程模型}
    
Saltzer 于 1966 年完善了进程作为虚拟处理器的形式化定义 \cite{saltzer1966traffic}，Dijkstra 在 THE 系统中首次提出“协同顺序进程”概念 \cite{dijkstra1968structure}。进程模型作为现代操作系统的核心抽象，借助了硬件内存管理单元（MMU，Memory Manage Unit）提供的地址空间隔离机制，保证了不同进程的独立性和安全性。

进程模型引入了状态机模型，其基础的三状态模型包括就绪态（已获资源但等待 CPU）、运行态（占用 CPU 执行指令）和阻塞态（因等待 I/O 等事件暂停执行），扩展的五状态模型增加了创建态（分配初始资源）和终止态（资源回收），而七状态模型进一步引入了挂起状态（如就绪挂起和阻塞挂起），用于处理内存不足时进程被换出到磁盘的情况。状态转换由事件触发，例如运行态因为时间片耗尽转为就绪态，或主动请求资源进入阻塞态，待事件完成后再恢复就绪态。

进程模型使用进程控制块（PCB，Process Control Block）来描述进程的状态以及对系统资源的占用情况（包括程序计数器、通用寄存器、内存映像、文件描述符、进程 ID 等信息），并将 PCB 作为任务的唯一标识以及任务调度的基本单位，操作系统通过管理 PCB 来实现进程的创建、调度、终止等操作。UNIX V7 采用多级反馈队列（MLFQ）算法，结合固定时间片轮转与优先级抢占机制，将缩短了任务平均周转时间 \cite{ritchie1974unix}，但此时进程既是资源容器（拥有文件描述符、内存映像等），又是调度实体，但重量级上下文切换制约了细粒度并发。

\subsection{线程模型}

由于硬件提供的地址空间隔离机制，进程间通信（IPC，Inter-Process Communication）需要依赖一些复杂机制（例如管道、套接字或共享内存），其上下文切换涉及特权级切换、地址空间切换等操作，导致通信开销显著增大，例如，共享内存通信需要缓存一致性协议来保证数据同步，而消息传递模型则需要多次复制数据。并且，由于进程同时是资源分配的单位，创建和销毁的开销也较大，这些问题促使多线程模型的出现。

线程模型则将执行单元与资源所有权解耦，进程仍然为系统资源分配的单位，线程为执行和任务调度的单元，共享进程的资源（如内存和文件句柄等），降低通信的开销，每个线程在线程控制块（TCB，Thread Control Block）中维护了状态信息（包括程序计数器、通用寄存器等）。这些创新共同促成调度粒度从进程级的毫秒量级向线程级的微秒精度迈进。并且，硬件上的多核处理器技术（例如对称多处理器和非对称多处理器）\footnote{多个物理处理器之间共享内存子系统以及总线结构等。}以及超线程技术\footnote{单个物理核心模拟多个逻辑核心，实现指令级并行与资源复用。}进一步推动了多线程模型的发展，进一步提高了计算机系统的性能。

多线程模型根据其实现方式可以分为内核级线程（$1:1$ 模型，每个用户态线程对应一个内核态线程）、用户级线程（$N:1$，多个用户态线程对应一个内核态线程）以及两者的混合模型（$M:N$，将 $M$ 个用户态线程映射到 $N$ 个内核态线程）。内核级线程由操作系统内核直接管理，线程的创建、调度、销毁等操作都由内核完成，因此线程切换的开销较大，但能够充分利用多核处理器的并行性。用户级线程则由用户空间的线程库管理，线程的创建、调度、销毁等操作都由用户空间的线程库完成，因此线程切换的开销较小，但无法利用多核处理器的并行性，单个线程阻塞会导致其他线程无法执行。混合模型则结合两者优势，既降低开销又支持多核并行。

然而，线程模型需要额外的同步互斥机制（如互斥锁、信号量）来管理共享的进程资源，避免数据竞争，这带来了额外的开销。例如，互斥锁的获取与释放操作在竞争激烈场景下可能导致线程阻塞，而自旋锁虽减少上下文切换但会导致 CPU 空转。

\subsection{协程模型}

协程（coroutine）的概念早已经被提出，Marlin 在他的博士论文中总结了协程的核心特征\cite{marlin1980coroutines}：
\begin{itemize}
    \item 协程的本地数据在连续调用之间不变；
    \item 当协程失去控制权时，其暂停执行；在协程重新获取控制权后，协程会从上一次暂停的地方恢复执行；
\end{itemize}
但是这个通用的定义却没有解决协程结构的相关问题，影响了编程语言中对于协程的支持方式，其中一些实现对协程的表达能力存在误解，此外，一等延续（first-class continuations\footnote{continuation 是计算机程序控制状态的抽象表示，它实现了程序控制状态，即 continuation 是一个表示程序执行中给定点的计算过程的数据结构；所创建的数据结构可以被编程语言访问，而不是隐藏在运行时环境中。}）的引入和多线程作为并发编程的“事实标准”采纳，导致协程没有得到广泛使用。直到 Moura 等人证明了完全协程（full coroutines）具有和一次性续延（one-shot continuations）和一次性限定续延（one-shot delimited continuations）同等的表达能力，协程才逐渐复兴 \cite{moura2009revisiting}。如今，现代编程语言（如 C++、Go、Rust、Python、Kotlin 等）都提供了不同程度的协程支持。

协程作为协作式任务单元，其无栈式设计（Stackless）相较于进（线）程模型，占用的内存资源更少。协程通过 CPS（continuation-passing style）\footnote{是一种通过显式传递“后续操作”来控制程序执行流程的编程范式。其核心思想是将函数原本隐式返回值的逻辑，替换为将结果传递给一个显式的回调函数（称为 Continuation）。}或者状态机保存执行现场，将上下文切换开销降至纳秒级，并且其与 IO 多路复用、事件驱动等机制非常契合，协程的这些特性展现出在高并发场景下的潜力，现代的运行时甚至能够每 GB 内存承载百万级轻量协程。

\section{任务调度}

Linux 内核中的 $O(1)$ 调度器设计突破了传统 $O(n)$ 调度器的性能瓶颈，通过创新性的双队列轮转机制实现了时间复杂度优化，$O(1)$ 调度器采用基于优先级的分层队列架构，将可运行任务划分为 $active$（活跃队列）和 $expired$（过期队列）两个独立集合。当任务的时间片耗尽时，调度器会即时执行优先级重计算，而非等待所有任务时间片耗尽后才统一处理，这一设计显著降低了调度决策的时间复杂度。而 Linux 内核使用的公平调度算器（CFS，Completely Fair Scheduler）为了保证公平性，赋予任务虚拟运行时间（$vruntime$）的概念，保证了在任意调度周期内，所有任务 $vruntime$ 的累积增量相等，从而实现公平调度。
% \begin{itemize}
%     \item 静态优先级 $nice_{i}$（$nice_{0}$ 表示静态优先级为 0 的数值）
%     \item 权重 $\omega_{i}$（由 $nice_{i}$ 决定）
%     \item 虚拟运行时间 $vruntime_{i}$（$vruntime_{i} = vruntime_{i} + \varDelta t \cdot \frac{nice_{0}}{\omega_{i} }$，$\varDelta t$ 为任务实际运行的时间片）
% \end{itemize}
% 在调度周期 $\varGamma $ 内，任务 $T_{i}$ 分配的实际运行时间 $t_{i} = \varGamma \cdot \frac{\omega_{i} }{\sum_{j \in \mathcal{R} } \omega_{j}  } $，。它的任务队列 $RQ$ 按照 $vruntime$ 排序：
% \begin{equation*}
%     RQ = \{ T_{1}, T_{2}, \ldots, T_{n} \}, vruntime(T_{1}) \leq vruntime(T_{2}) \leq \ldots \leq vruntime(T_{n})
% \end{equation*}
% 其调度决策为 $next=\arg \min_{T_{i}\in RQ} vruntime(T_{i})$。通过这种建模，CFS 调度算法保证了在任意调度周期 $\varGamma $ 内，所有任务 $vruntime$ 的累积增量相等（$\sum_{i\in \mathcal{R}} vruntime_{i}(T) = nice_{0}\cdot \varGamma $）。

在任务调度算法的建模和优化领域，现有研究通常围绕系统核心性能指标展开针对性设计。以 Concord \cite{iyer2023achieving}为例，它通过构建涵盖抢占式调度、线程同步与通信、任务分发等环节的开销模型，对系统尾部延时的生成路径进行解耦分析，最终在保障吞吐量的同时将尾部延时优化至微秒级。而葛文博等人提出的众核嵌入式实时调度策略，引入了任务关键度概念，建立任务最坏响应时间和优先级分配的数学模型，有效的降低了传统调度在众核环境下的调度开销 \cite{gewenbo2023}。

然而，现有研究多聚焦于单一或有限维度的性能指标建模，在动态可重构的系统中，这些确定性的模型与动态场景的适配不足，难以适应复杂应用场景的多样化需求。

\section{I/O 模型}

I/O 模型描述了应用程序与外部设备之间通信的过程，应用程序通过系统调用来执行 I/O 操作，内核来执行具体的 I/O 操作。I/O 模型设计面临的核心问题是 CPU 与外部设备速度不匹配，不同的模型通过不同的方式协调这一矛盾：（1）同步与异步：同步模型要求应用程序主动轮询或等待结果，而异步模型则由内核主动通知应用程序；（2）阻塞与非阻塞：阻塞模型会挂起线程直到操作完成，而非阻塞模型则允许线程立即返回并执行其它任务。常见的 I/O 模型包括以下几类：

\begin{itemize}
    \item 同步阻塞 I/O：应用程序发起 I/O 操作后，线程会完全阻塞，直到内核完成数据准备和拷贝到用户空间两阶段操作。例如发起 \verb|read()| 系统调用时，若内核缓冲区无数据，线程进入休眠状态，直到数据到达后被唤醒。这个模型逻辑简单直观，但线程资源利用率较低，适用于低并发的场景，在高并发时需要创建大量线程而导致内存与调度开销增大。
    
    \item 同步非阻塞 I/O：应用程序通过 \verb|fcntl()| 系统调用将文件描述符设置为非阻塞模式，当发起 \verb|read()| 系统调用时，若内核无数据，立即返回 \verb|EWOULDBLOCK| 错误，不会将线程阻塞在内核中，线程回到用户态可以执行其他操作，但仍然需要定期轮询 I/O 状态。这种模式减少了线程空闲等待的时间，提升了单线程利用率，但定期轮询导致了 CPU 空转，存在资源浪费，并且受到轮询间隔的影响，在数据准备就绪到应用程序进行处理存在一定的响应延时。
    
    \item I/O 多路复用：单个线程通过 \verb|select()|、\verb|poll()|、\verb|epoll()| 等系统调用，阻塞在内核中，内核会监控多个文件描述符的 I/O 状态，当其中某个文件描述符就绪时，系统调用就会返回到用户态，线程可以执行 I/O 操作。这种模型适用于大量 I/O 事件的场景，避免了线程创建和销毁的开销，但这些机制需要内核维护额外的数据结构以及在内核与用户程序之间拷贝文件描述符集合，存在着一定开销。
    
    \item 异步 I/O：应用程序通过异步读/写接口发起 i/O 操作后，立即从内核态回到用户态执行其他操作，当 I/O 事件准备就绪，内核将数据拷贝至应用程序的缓冲区中，内核发送一个信号或者或者基于线程的回调函数来完成 I/O 处理。以信号机制作为异步通知机制为例，应用程序使用 \verb|sigaction()| 系统调用注册 \verb|SIGIO| 信号后，回到用户态执行其他的任务，当 I/O 准备就绪时，内核向应用程序发送 \verb|SIGIO| 信号，当应用程序下一次进入内核并返回到用户态时，会进入预先注册的信号处理函数中，完成 I/O 事件的处理，在信号处理函数尾部，通过 \verb|sigreturn()| 系统调用使应用程序从被打断处恢复执行。这种机制避免了轮询造成的 CPU 开销，但由于信号处理函数需要等到应用程序下一次进入内核并返回时才能执行，这与同步非阻塞 I/O 模型存在着类似的响应延时，并且增加了额外的系统调用开销。此外还存在其他的异步通知机制，例如用户态中断技术。异步 I/O 模型通过解耦 I/O 操作与程序执行流，突破了传统同步模型的性能瓶颈，在构建高并发的云服务场景下效果显著。
    
\end{itemize}

在选择 I/O 模型时，需综合考虑应用场景的负载类型、性能需求、并发规模及系统资源限制等因素。例如，高并发短连接场景（如 Web 服务器）通常采用 I/O 多路复用模型以高效管理大量连接，而实时系统或响应式界面更适合非阻塞 I/O 以避免阻塞延迟；对于简单低负载应用（如嵌入式设备），阻塞式 I/O 因其实现简单且资源消耗低可能是更优选择。此外，跨平台兼容性、开发复杂度（如回调机制）和操作系统特性（如 Linux 对异步 I/O 的模拟实现）等因素也需要考虑。

\section{用户态中断}

用户态中断（User-Level Interrupts）作为近年来计算机体系结构和操作系统领域的研究热点，旨在绕过传统内核态中断处理的高开销，通过硬件与软件的协同设计实现用户态程序的低延迟异步事件响应。其核心思想是通过硬件直接传递中断信号到用户空间，并允许用户态程序注册自定义的中断处理逻辑，从而消除传统的由内核处理中断机制导致的上下文切换开销（特权级切换开销，在开启内核页表隔离机制（KPTI）\footnote{内核页表隔离机制是 Linux 内核的一项功能，它将用户空间和内核空间页表完全分离来强化内核安全性。}后，还包括地址空间切换开销）。

目前已经存在多项工作围绕着用户态中断技术展开，Skyloft\cite{jia2024skyloft}、uProcess\cite{Linjiazhen2024} 使用了 Intel 提出的 UINTR（User Interrupts）扩展 \cite{x86_uintr} 来实现用户态的抢占式调度。此外，RISC-V 指令集 \cite{riscv} 的 N 扩展也实现了对用户态中断技术的支持，Sandro 等人已经将其用于嵌入式系统，用于构建可信执行环境\cite{pintouser}。

用户态中断技术除了可以为实现用户态抢占式调度提供了硬件原生支持，还可通过内核旁路（Kernel Bypass）和零拷贝（Zero-Copy）机制显著加速 IPC。用户态中断充当通知机制，中断信号直接触发用户态处理逻辑，并且通过共享内存或其他机制绕过多层内核协议栈与内存拷贝操作，从而消除传统 IPC 中因系统调用、上下文切换和数据复制导致的开销。这对于高性能通信具有重要影响，但用户态中断技术也面临着安全隔离性（如恶意中断注入、中断嵌套）、跨平台兼容性（需依赖特定 CPU 架构）以及调试复杂性（缺乏内核调试接口）等挑战。

\section{本章小结}

本章围绕中断响应与任务调度的相关技术展开探讨。首先从硬件体系结构的迭代升级切入，揭示了计算机系统如何通过软硬件协同设计推动任务模型的持续演进——从早期的单任务串行执行逐步发展为支持多任务抢占式调度的复杂架构；然后从不同应用场景对延时、吞吐量等性能指标的需求展开对任务调度算法以及 I/O 模型的讨论，最后介绍了用户态中断技术的研究现状。这些技术的发展和应用为本文后续的研究工作提供了理论基础和技术支持，尤其是用户态中断技术，它打破了内核与用户态的壁垒，推动了软硬件深度融合的进程。
